diff --git a/Makefile b/Makefile
index 759e68a02..371e62a6e 100644
--- a/Makefile
+++ b/Makefile
@@ -1115,7 +1115,7 @@ export MODORDER := $(extmod_prefix)modules.order
 export MODULES_NSDEPS := $(extmod_prefix)modules.nsdeps
 
 ifeq ($(KBUILD_EXTMOD),)
-core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/
+core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ ../../project/memstat_syscall/
 
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 18b5500ea..01e6b32d3 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -413,5 +413,8 @@
 545	x32	execveat		compat_sys_execveat
 546	x32	preadv2			compat_sys_preadv64v2
 547	x32	pwritev2		compat_sys_pwritev64v2
+
+548	64	memstat_syscall		sys_memstat_syscall
 # This is the end of the legacy x32 range.  Numbers 548 and above are
 # not special and are not to be used for x32-specific syscalls.
+
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 766ffe3ba..20aa3de54 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -29,6 +29,8 @@
 DEFINE_PER_CPU_SHARED_ALIGNED(irq_cpustat_t, irq_stat);
 EXPORT_PER_CPU_SYMBOL(irq_stat);
 
+EXPORT_SYMBOL_GPL(vector_irq);
+
 atomic_t irq_err_count;
 
 /*
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 7f8ee09c7..f8c08d522 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -237,6 +237,7 @@ struct page {
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
+	int my_ref_c;
 } _struct_page_alignment;
 
 static inline atomic_t *compound_mapcount_ptr(struct page *page)
diff --git a/include/linux/swap.h b/include/linux/swap.h
index ba52f3a34..e5ac1c155 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -21,7 +21,11 @@ struct notifier_block;
 struct bio;
 
 struct pagevec;
-
+//
+extern int promote_count;
+extern int demote_count;
+extern int evict_count;
+//
 #define SWAP_FLAG_PREFER	0x8000	/* set if swap priority specified */
 #define SWAP_FLAG_PRIO_MASK	0x7fff
 #define SWAP_FLAG_PRIO_SHIFT	0
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 252243c77..9b5838043 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1272,6 +1272,8 @@ asmlinkage long sys_old_mmap(struct mmap_arg_struct __user *arg);
  */
 asmlinkage long sys_ni_syscall(void);
 
+asmlinkage long sys_memstat_syscall(int*);
+
 #endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */
 
 
diff --git a/init/main.c b/init/main.c
index bcd132d4e..8e582dc6c 100644
--- a/init/main.c
+++ b/init/main.c
@@ -932,6 +932,7 @@ static void __init print_unknown_bootoptions(void)
 
 asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 {
+	printk(KERN_INFO "Hello, World\n");
 	char *command_line;
 	char *after_dashes;
 
diff --git a/kernel/irq/irqdesc.c b/kernel/irq/irqdesc.c
index 4e3c29bb6..90241051c 100644
--- a/kernel/irq/irqdesc.c
+++ b/kernel/irq/irqdesc.c
@@ -352,6 +352,7 @@ struct irq_desc *irq_to_desc(unsigned int irq)
 {
 	return radix_tree_lookup(&irq_desc_tree, irq);
 }
+EXPORT_SYMBOL_GPL(irq_to_desc);
 #ifdef CONFIG_KVM_BOOK3S_64_HV_MODULE
 EXPORT_SYMBOL_GPL(irq_to_desc);
 #endif
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 23d3339ac..208d5eb08 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1567,6 +1567,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
 	page_kasan_tag_reset(page);
+	page->my_ref_c = 0;
 
 	INIT_LIST_HEAD(&page->lru);
 #ifdef WANT_PAGE_VIRTUAL
diff --git a/mm/swap.c b/mm/swap.c
index af3cad4e5..00420bf97 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -42,7 +42,11 @@
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/pagemap.h>
-
+//
+int promote_count;
+// int demote_count;
+// int evict_count;
+//
 /* How many pages do we try to swap or page in/out together? */
 int page_cluster;
 
@@ -418,8 +422,10 @@ void mark_page_accessed(struct page *page)
 		 * pagevec, mark it active and it'll be moved to the active
 		 * LRU on the next drain.
 		 */
-		if (PageLRU(page))
+		if (PageLRU(page)) {
+			promote_count++;
 			activate_page(page);
+		}
 		else
 			__lru_cache_activate_page(page);
 		ClearPageReferenced(page);
@@ -667,8 +673,9 @@ void deactivate_page(struct page *page)
 		local_lock(&lru_pvecs.lock);
 		pvec = this_cpu_ptr(&lru_pvecs.lru_deactivate);
 		get_page(page);
-		if (pagevec_add_and_need_flush(pvec, page))
+		if (pagevec_add_and_need_flush(pvec, page)) {
 			pagevec_lru_move_fn(pvec, lru_deactivate_fn);
+		}
 		local_unlock(&lru_pvecs.lock);
 	}
 }
@@ -941,6 +948,7 @@ void release_pages(struct page **pages, int nr)
 		}
 
 		if (PageLRU(page)) {
+			// evict_count++;
 			struct lruvec *prev_lruvec = lruvec;
 
 			lruvec = relock_page_lruvec_irqsave(page, lruvec,
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 74296c2d1..8fc04e2b6 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -61,7 +61,10 @@
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/vmscan.h>
-
+//
+int evict_count;
+int demote_count;
+//
 struct scan_control {
 	/* How many pages shrink_list() should reclaim */
 	unsigned long nr_to_reclaim;
@@ -2320,6 +2323,7 @@ shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,
 
 	trace_mm_vmscan_lru_shrink_inactive(pgdat->node_id,
 			nr_scanned, nr_reclaimed, &stat, sc->priority, file);
+	evict_count += nr_reclaimed;
 	return nr_reclaimed;
 }
 
@@ -2348,6 +2352,8 @@ static void shrink_active_list(unsigned long nr_to_scan,
 	unsigned long nr_taken;
 	unsigned long nr_scanned;
 	unsigned long vm_flags;
+	// printk(KERN_INFO "t\n");
+
 	LIST_HEAD(l_hold);	/* The pages which were snipped off */
 	LIST_HEAD(l_active);
 	LIST_HEAD(l_inactive);
@@ -2407,7 +2413,7 @@ static void shrink_active_list(unsigned long nr_to_scan,
 				continue;
 			}
 		}
-
+		demote_count++;
 		ClearPageActive(page);	/* we are de-activating */
 		SetPageWorkingset(page);
 		list_add(&page->lru, &l_inactive);
@@ -2490,7 +2496,6 @@ unsigned long reclaim_pages(struct list_head *page_list)
 	}
 
 	memalloc_noreclaim_restore(noreclaim_flag);
-
 	return nr_reclaimed;
 }
 
@@ -3594,10 +3599,41 @@ static bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,
 	return false;
 }
 
+void relocate_least_freq(struct list_head *src){
+   struct page *iter_page ;
+   struct page *victim_page;
+   struct list_head *victim_head;
+   struct list_head *i, *tmp;
+   int iternum=0;
+
+   list_for_each_safe(i, tmp, src) {
+      if(iternum ==0){
+         victim_head = i;
+         victim_page = iter_page;
+      }
+      else{
+         if(iter_page->my_ref_c < victim_page->my_ref_c){
+            victim_head = i;
+            victim_page = iter_page;
+         }
+      }
+      iternum ++;
+   }
+   list_del_init(i);
+   list_add_tail(i, src);
+
+}
+
 unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 				gfp_t gfp_mask, nodemask_t *nodemask)
 {
 	unsigned long nr_reclaimed;
+	struct pglist_data *current_pglist = NULL;
+   struct lruvec *lruvec = NULL;
+   struct mem_cgroup *memcg = NULL;
+   struct mem_cgroup_per_node *mz;
+   int i;
+
 	struct scan_control sc = {
 		.nr_to_reclaim = SWAP_CLUSTER_MAX,
 		.gfp_mask = current_gfp_context(gfp_mask),
@@ -3610,6 +3646,29 @@ unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 		.may_swap = 1,
 	};
 
+	for (i = 0; i < MAX_NUMNODES; i++) {
+      if (NODE_DATA(i) == NULL)
+         continue;
+ 
+      current_pglist = NODE_DATA(i);
+ 
+      if (current_pglist->node_present_pages == 0) {
+         printk(KERN_ALERT "Node-%d does not have any pages.\n", i);
+         continue;
+      }
+
+      memcg = get_mem_cgroup_from_mm(NULL);
+      lruvec = mem_cgroup_lruvec(memcg, current_pglist);
+ 
+      spin_lock_irq(&lruvec->lru_lock);
+      relocate_least_freq(&lruvec->lists[LRU_ACTIVE_FILE]);
+      relocate_least_freq(&lruvec->lists[LRU_INACTIVE_FILE]);
+      relocate_least_freq(&lruvec->lists[LRU_ACTIVE_ANON]);
+      relocate_least_freq(&lruvec->lists[LRU_INACTIVE_ANON]);
+      relocate_least_freq(&lruvec->lists[LRU_UNEVICTABLE]);
+      spin_unlock_irq(&lruvec->lru_lock);
+   	}
+
 	/*
 	 * scan_control uses s8 fields for order, priority, and reclaim_idx.
 	 * Confirm they are large enough for max values.
diff --git a/my_syscall/Makefile b/my_syscall/Makefile
new file mode 100644
index 000000000..1cd186470
--- /dev/null
+++ b/my_syscall/Makefile
@@ -0,0 +1 @@
+obj-y += my_func.o
diff --git a/my_syscall/my_func.c b/my_syscall/my_func.c
new file mode 100644
index 000000000..205066a8a
--- /dev/null
+++ b/my_syscall/my_func.c
@@ -0,0 +1,50 @@
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+
+SYSCALL_DEFINE2(my_syscall, int, n, int*, ret_prime_nums) 
+{
+	int* nums = kmalloc(sizeof(int) * n, GFP_KERNEL);
+	int temp = 0, prime_nums = 0, num;
+	int i=0, j=0, flag=1;
+	int *prime_nums_ptr = kmalloc(sizeof(int), GFP_KERNEL);
+
+	if(nums == NULL) {
+		printk(KERN_INFO "kmalloc Error!\n");
+		return 1;
+	}
+
+	for(i=0; i<n; i++) {
+		get_random_bytes(&temp, sizeof(int)-1);
+		nums[i] = temp % n;
+	}
+	
+	for(i=0; i<n; i++) {
+		num = nums[i];
+		if(num < 2) continue; 
+		
+		for(j=2; j*j<=num; j++) {
+			if(num % j == 0) {
+				flag = 0;
+				break;
+			}
+		}
+		if(flag) {
+			prime_nums++;
+		}
+		else flag = 1;
+	}
+	*prime_nums_ptr = prime_nums;
+	
+	if(copy_to_user(ret_prime_nums, prime_nums_ptr, sizeof(int)) != 0) {
+		printk(KERN_INFO "copy_to_user Error!\n");
+		return 1;
+	}
+	
+	kfree(prime_nums_ptr);
+	kfree(nums);
+	return 0;
+
+}
+
diff --git a/prime_syscall/Makefile b/prime_syscall/Makefile
new file mode 100644
index 000000000..6e50ac42a
--- /dev/null
+++ b/prime_syscall/Makefile
@@ -0,0 +1 @@
+obj-y += prime_sys.o
diff --git a/prime_syscall/prime_sys.c b/prime_syscall/prime_sys.c
new file mode 100644
index 000000000..b7abd9e92
--- /dev/null
+++ b/prime_syscall/prime_sys.c
@@ -0,0 +1,50 @@
+#include <linux/kernel.h>
+#include <linux/syscalls.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+
+SYSCALL_DEFINE2(prime_syscall, int, n, int*, ret_prime_nums) 
+{
+	int* nums = kmalloc(sizeof(int) * n, GFP_KERNEL);
+	int temp = 0, prime_nums = 0, num;
+	int i=0, j=0, flag=1;
+	int *prime_nums_ptr = kmalloc(sizeof(int), GFP_KERNEL);
+
+	if(nums == NULL) {
+		printk(KERN_INFO "kmalloc Error!\n");
+		return 1;
+	}
+
+	for(i=0; i<n; i++) {
+		get_random_bytes(&temp, sizeof(int)-1);
+		nums[i] = temp % n;
+	}
+	
+	for(i=0; i<n; i++) {
+		num = nums[i];
+		if(num < 2) continue; 
+		
+		for(j=2; j*j<=num; j++) {
+			if(num % j == 0) {
+				flag = 0;
+				break;
+			}
+		}
+		if(flag) {
+			prime_nums++;
+		}
+		else flag = 1;
+	}
+	*prime_nums_ptr = prime_nums;
+	
+	if(copy_to_user(ret_prime_nums, prime_nums_ptr, sizeof(int)) != 0) {
+		printk(KERN_INFO "copy_to_user Error!\n");
+		return 1;
+	}
+	
+	kfree(prime_nums_ptr);
+	kfree(nums);
+	return 0;
+
+}
+
